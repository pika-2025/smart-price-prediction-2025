{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13361781,"sourceType":"datasetVersion","datasetId":8475403},{"sourceId":13367962,"sourceType":"datasetVersion","datasetId":8477994}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment & Data Setup\n\nBefore building the model, we need to configure our notebook environment for high-performance processing and load our data.\n\n#### 1. Data Configuration\n* **Kaggle Datasets**: All project data (the `train.csv`, `test.csv`, and the `images/` directory) was first uploaded as a private Kaggle Dataset.\n* **Notebook Import**: We then imported this dataset into the notebook. This is the most efficient way to handle large files on Kaggle, as the data is mounted directly to the notebook's filesystem under the `/kaggle/input/` directory.\n\n#### 2. Accelerator: GPU T4 x2\n* **Enabled GPU**: To dramatically speed up our feature extraction, we enabled a high-performance GPU accelerator. This can be done in the notebook's right-hand menu:\n    `Settings` -> `Accelerator` -> `GPU T4 x2`\n* **Why T4 x2?**: This option provides us with **two** NVIDIA T4 GPUs. For an \"embarrassingly parallel\" task like `model.predict()`, this is ideal. By using a distribution strategy (like `tf.distribute.MirroredStrategy`), TensorFlow can automatically split the prediction workload across both GPUs, processing two batches of images simultaneously. This effectively **halves the time** required for feature extraction compared to a single GPU.","metadata":{}},{"cell_type":"markdown","source":"# Importing the Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport time\nfrom PIL import Image\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport joblib\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom scipy.sparse import csr_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:55:05.547373Z","iopub.execute_input":"2025-10-13T15:55:05.547541Z","iopub.status.idle":"2025-10-13T15:55:20.105347Z","shell.execute_reply.started":"2025-10-13T15:55:05.547525Z","shell.execute_reply":"2025-10-13T15:55:20.104537Z"}},"outputs":[{"name":"stderr","text":"2025-10-13 15:55:07.557668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760370907.798538      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760370907.873522      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Importing Dataset","metadata":{}},{"cell_type":"code","source":"DATASET_FOLDER = '/kaggle/input/amazon-ml-data'\ntrain = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\ntest = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:59:07.210798Z","iopub.execute_input":"2025-10-13T16:59:07.211289Z","iopub.status.idle":"2025-10-13T16:59:10.705543Z","shell.execute_reply.started":"2025-10-13T16:59:07.211268Z","shell.execute_reply":"2025-10-13T16:59:10.704763Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Load Pre-trained Base Model (MobileNetV2)\n\nWe initialize the **MobileNetV2** model, pre-trained on the ImageNet dataset.\n\n* `input_shape=(224, 224, 3)`: Specifies the input size for our images (224x224 pixels with 3 color channels).\n* `include_top=False`: Excludes the final fully-connected (classification) layer from the original MobileNetV2. This allows us to add our own custom classifier suited for our specific task.\n* `weights='imagenet'`: Loads weights pre-trained on the ImageNet dataset, which is crucial for transfer learning.\n* `pooling='avg'`: Applies **Global Average Pooling** to the output of the base model. This converts the feature maps into a single flat vector per image, making it easy to connect to our new classification layer.","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.MobileNetV2(\n        input_shape=(224, 224, 3),\n        include_top=False,\n        weights='imagenet',\n        pooling='avg' # Applies Global Average Pooling to the output\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:55:20.108702Z","iopub.execute_input":"2025-10-13T15:55:20.108877Z","iopub.status.idle":"2025-10-13T15:55:23.322170Z","shell.execute_reply.started":"2025-10-13T15:55:20.108859Z","shell.execute_reply":"2025-10-13T15:55:23.321545Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1760370921.021442      37 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1760370921.022293      37 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Load Image Data using `tf.data`\n\nWe use the `tf.keras.utils.image_dataset_from_directory` utility to create a `tf.data.Dataset` object. This is a highly efficient way to load images from a directory directly into a TensorFlow data pipeline.\n\n* `root_dir`: The path to our images.\n* `labels=None` / `label_mode=None`: We explicitly tell the function *not* to look for labels. This is because our images are not organized into subfolders by class (e.g., `/dogs`, `/cats`). We will get our labels from a separate CSV file.\n* `batch_size=64`: Loads the images in batches of 64.\n* `image_size=(224, 224)`: Resizes all images to 224x224 to match the input shape required by our MobileNetV2 model.\n* `shuffle=False`: This is **critical**. We must not shuffle the dataset here so that the order of the images loaded from the directory perfectly matches the order of the corresponding data (like labels or IDs) in our `train.csv` or `test.csv` file.\n* `interpolation='lanczos5'`: Uses a high-quality resampling filter for resizing the images.\n\nFinally, we apply `.prefetch(buffer_size=tf.data.AUTOTUNE)`. This is a performance optimization that allows the CPU to pre-load the next batch of images while the GPU is busy processing the current one, preventing data bottlenecks.","metadata":{}},{"cell_type":"code","source":"root_dir = \"/kaggle/input/amazon-ml-image-data/images/\"\nimage_dataset = tf.keras.utils.image_dataset_from_directory(\n            root_dir,\n            labels=None,\n            label_mode=None,\n            batch_size=64,\n            image_size=(224, 224),\n            shuffle=False, # CRITICAL for aligning with external data\n            interpolation='lanczos5'\n        )\nimage_dataset = image_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:55:23.323362Z","iopub.execute_input":"2025-10-13T15:55:23.323587Z","iopub.status.idle":"2025-10-13T15:59:07.577984Z","shell.execute_reply.started":"2025-10-13T15:55:23.323570Z","shell.execute_reply":"2025-10-13T15:59:07.577357Z"}},"outputs":[{"name":"stdout","text":"Found 72284 files.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Extract Features (Generate Embeddings)\n\nWe now pass our entire `image_dataset` through the pre-trained `base_model` using the `.predict()` method.\n\n* This is the main step of **feature extraction**.\n* Since we set `include_top=False` and `pooling='avg'` when loading MobileNetV2, the model doesn't output final predictions. Instead, it processes each image and converts it into a high-level feature vector (also known as an \"embedding\").\n* For each image in the dataset, the output will be a 1D vector of **1280 features**.\n\nThe `extracted_features` variable will now hold a large NumPy array with the shape `(total_number_of_images, 1280)`. This new array represents our image data in a numerical format that we can easily use to train a simpler machine learning model (like XGBoost, LightGBM, or a small, dense neural network).","metadata":{}},{"cell_type":"code","source":"extracted_features = base_model.predict(image_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:00:08.573912Z","iopub.execute_input":"2025-10-13T16:00:08.574800Z","iopub.status.idle":"2025-10-13T16:11:48.120640Z","shell.execute_reply.started":"2025-10-13T16:00:08.574770Z","shell.execute_reply":"2025-10-13T16:11:48.119503Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1760371211.129387     100 service.cc:148] XLA service 0x7f6b7820fdd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1760371211.130327     100 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1760371211.130348     100 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1760371211.592547     100 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m   3/1130\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 45ms/step  ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1760371216.674549     100 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1010/1130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1:22\u001b[0m 684ms/step","output_type":"stream"},{"name":"stderr","text":"Input file read error\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3940985135.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextracted_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6001\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6002\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} jpeg::Uncompress failed. Invalid JPEG data or crop window.\n\t [[{{node decode_image/DecodeImage}}]] [Op:IteratorGetNext] name: "],"ename":"InvalidArgumentError","evalue":"{{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} jpeg::Uncompress failed. Invalid JPEG data or crop window.\n\t [[{{node decode_image/DecodeImage}}]] [Op:IteratorGetNext] name: ","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":"# Debugging: Finding Corrupted Images After specified Batch processing\n\nDuring the feature extraction, the `model.predict()` process failed at a specific batch (`Batch 1009`). This almost always indicates that one of the image files in that batch is corrupted or truncated.\n\nThis script is a targeted debugger to find the exact problematic file without having to re-scan the entire dataset from the beginning.\n\nHere is the logic:\n\n1.  **Configuration**: We set the `FAILED_AT_BATCH` variable to the batch number where the error occurred (1009) and note the `BATCH_SIZE` (64).\n2.  **Calculate Offset**: We calculate a `start_index` to skip all files from the batches that we know completed successfully (i.e., batches 1 through 1008). This is the key optimization.\n3.  **Gather & Sort Files**: The script gets a complete list of all image paths from the directory.\n4.  **Critical Sort**: It performs an alphabetical sort on the file list. This is **essential** to ensure the list's order perfectly matches the order used by `tf.keras.utils.image_dataset_from_directory` (when `shuffle=False`).\n5.  **Targeted Scan**: It creates a new, smaller list (`files_to_check`) containing only the \"suspect\" files, starting from the calculated `start_index`.\n6.  **Verify with TensorFlow**: It iterates *only* through this suspect list and attempts to open and decode each file using TensorFlow's own I/O functions (`tf.io.read_file` and `tf.io.decode_image`). This is crucial because it finds the *exact* file that TensorFlow considers corrupt, which other libraries (like PIL) might not.\n\nAny file that throws an exception during this process is identified as the corrupted file.","metadata":{}},{"cell_type":"code","source":"image_dir = '/kaggle/input/amazon-ml-image-data/images/'\nBATCH_SIZE = 64\nFAILED_AT_BATCH = 1009 # This is the batch number where the error occurred\n\n\n# 1. Calculate the starting index\n# We skip the files from all the batches that completed successfully\nstart_index = (FAILED_AT_BATCH - 1) * BATCH_SIZE\nprint(f\"Calculation: ({FAILED_AT_BATCH} - 1) * {BATCH_SIZE} = {start_index}\")\nprint(f\"Will start scanning for bad files after skipping the first {start_index} files.\")\n\n# 2. Get a complete, sorted list of all image files\nprint(\"Gathering and sorting all file paths...\")\nall_files = []\nfor root, dirs, files in os.walk(image_dir):\n    for filename in files:\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n            all_files.append(os.path.join(root, filename))\n\n# **CRITICAL**: You must sort the list to match the order TensorFlow uses.\nall_files.sort()\nprint(f\"Found a total of {len(all_files)} images.\")\n\n# 3. Slice the list to get only the files that need to be checked\nfiles_to_check = all_files[start_index:]\nprint(f\"Now checking the remaining {len(files_to_check)} files...\\n\")\n\n# 4. Run the check on the smaller, targeted list of files\nbad_files = []\nfor filepath in files_to_check:\n    try:\n        # Use TensorFlow's own functions to be certain\n        image_bytes = tf.io.read_file(filepath)\n        tf.io.decode_image(image_bytes)\n    except Exception as e:\n        print(f\"Corrupted file found: {filepath}\")\n        print(f\"   Reason: {e}\")\n        bad_files.append(filepath)\n        # Optional: stop after finding the first bad file\n        # break \n\nprint(\"\\n--- Scan Complete ---\")\nif bad_files:\n    print(f\"Found {len(bad_files)} corrupted files in the suspected range.\")\n    print(\"Remove these files from your dataset and any corresponding label files.\")\nelse:\n    print(\"No corrupted files found in the suspected range. The issue might be different.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:17:08.790448Z","iopub.execute_input":"2025-10-13T16:17:08.791041Z","iopub.status.idle":"2025-10-13T16:22:59.724353Z","shell.execute_reply.started":"2025-10-13T16:17:08.791006Z","shell.execute_reply":"2025-10-13T16:22:59.723631Z"}},"outputs":[{"name":"stdout","text":"Calculation: (1009 - 1) * 64 = 64512\n🎯 Will start scanning for bad files after skipping the first 64512 files.\nGathering and sorting all file paths...\nFound a total of 72284 images.\nNow checking the remaining 7772 files...\n\n","output_type":"stream"},{"name":"stderr","text":"Input file read error\n","output_type":"stream"},{"name":"stdout","text":"❌ Corrupted file found: /kaggle/input/amazon-ml-image-data/images/81stS23MRfL.jpg\n   Reason: {{function_node __wrapped__DecodeImage_device_/job:localhost/replica:0/task:0/device:CPU:0}} jpeg::Uncompress failed. Invalid JPEG data or crop window. [Op:DecodeImage] name: \n\n--- Scan Complete ---\nFound 1 corrupted files in the suspected range.\nRemove these files from your dataset and any corresponding label files.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Debugging & Cleaning: Finding All Corrupted Images\n\nAfter the `model.predict()` process failed at a specific batch (e.g., `1010`), it's clear we have one or more corrupted image files. Manually finding them in a large dataset is impossible.\n\nThis script is an optimized debugger designed to find and handle *all* problematic files from the point of failure onward.\n\n#### 1. Optimized Scanning\nInstead of re-checking the entire dataset (which could take hours), we first calculate a `start_index`. This index tells the script to skip all the files from the batches that we *know* processed successfully (e.g., batches 1 through 1009).\n\n#### 2. File Collection and Sorting\n* The script gathers all image paths from the directory.\n* **Critically**, it sorts the file list alphabetically (`all_files.sort()`). This is essential because it exactly matches the order that `tf.keras.utils.image_dataset_from_directory(shuffle=False)` uses to read the files.\n* It then slices this main list to create a smaller `files_to_check` list, containing only the \"suspect\" files from the point of failure.\n\n#### 3. TensorFlow-based Verification\nThe script loops through the suspect list and uses `tf.io.decode_image(..., channels=3)` to test each file. This is the most reliable method, as it uses the *exact* same decoding function that the TensorFlow model pipeline uses. Any file that causes an exception here is guaranteed to be a problem.\n\n#### 4. Collect & Remove\n* It finds and collects *all* corrupted files in the suspect range (it doesn't stop after the first one).\n* It prints a final summary list of all bad files found.\n* Finally, it attempts to `os.remove()` them. The `try...except` block correctly anticipates and handles the `OSError` (read-only filesystem error), which is expected when working in the `/kaggle/input/` directory. This serves to confirm which files *would* be removed if the directory were writable.","metadata":{}},{"cell_type":"code","source":"\nimage_dir = '/kaggle/input/amazon-ml-image-data/images/' \nBATCH_SIZE = 64\nFAILED_AT_BATCH = 1010 \n\nstart_index = (FAILED_AT_BATCH - 1) * BATCH_SIZE\nprint(f\"Configuration: Will start scanning after skipping the first {start_index} files.\")\n\nprint(\"Gathering and sorting all file paths...\")\nall_files = []\nfor root, dirs, files in os.walk(image_dir):\n    for filename in files:\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n            all_files.append(os.path.join(root, filename))\n\nall_files.sort()\nprint(f\"Found a total of {len(all_files)} images.\")\n\nfiles_to_check = all_files[start_index:]\nprint(f\"Now scanning {len(files_to_check)} files for corruption...\\n\")\n\ncorrupted_files_list = []\nfor filepath in files_to_check:\n    try:\n        image_bytes = tf.io.read_file(filepath)\n        tf.io.decode_image(image_bytes, channels=3) # Use channels=3 for consistency\n    except Exception as e:\n        print(f\"Found corrupted file: {filepath}\")\n        corrupted_files_list.append(filepath)\n\nprint(\"\\n\" + \"=\"*50)\nif not corrupted_files_list:\n    print(\"Scan complete. No corrupted files were found in the suspected range.\")\nelse:\n    print(f\"Found a total of {len(corrupted_files_list)} corrupted files.\")\n    print(\"--- List of Corrupted Files ---\")\n    for f in corrupted_files_list:\n        print(f)\n    \n    print(\"\\n--- Attempting to Remove Corrupted Files ---\")\n    for filepath in corrupted_files_list:\n        try:\n            os.remove(filepath)\n            print(f\"Successfully removed: {filepath}\")\n        except OSError as e:\n            print(f\"Error removing file: {filepath}\")\n            print(f\"   Reason: {e}\")\n            print(\"   This is expected in read-only environments like /kaggle/input/.\")\n\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:30:27.296548Z","iopub.execute_input":"2025-10-13T16:30:27.297249Z","iopub.status.idle":"2025-10-13T16:34:42.643867Z","shell.execute_reply.started":"2025-10-13T16:30:27.297215Z","shell.execute_reply":"2025-10-13T16:34:42.643073Z"}},"outputs":[{"name":"stdout","text":"🎯 Configuration: Will start scanning after skipping the first 64576 files.\nGathering and sorting all file paths...\nFound a total of 72284 images.\nNow scanning 7708 files for corruption...\n\n","output_type":"stream"},{"name":"stderr","text":"Input file read error\n","output_type":"stream"},{"name":"stdout","text":"❌ Found corrupted file: /kaggle/input/amazon-ml-image-data/images/81stS23MRfL.jpg\n\n==================================================\nFound a total of 1 corrupted files.\n--- List of Corrupted Files ---\n/kaggle/input/amazon-ml-image-data/images/81stS23MRfL.jpg\n\n--- 🗑️ Attempting to Remove Corrupted Files ---\n❗️ Error removing file: /kaggle/input/amazon-ml-image-data/images/81stS23MRfL.jpg\n   Reason: [Errno 30] Read-only file system: '/kaggle/input/amazon-ml-image-data/images/81stS23MRfL.jpg'\n   This is expected in read-only environments like /kaggle/input/.\n==================================================\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Building a Custom `tf.data` Pipeline to Skip Corrupted Files\n\nOur previous debugging step successfully identified a corrupted file (`81stS23MRfL.jpg`) that was causing the `model.predict()` process to fail.\n\nSince we cannot remove this file from the read-only `/kaggle/input/` directory, we must build a new data pipeline that manually filters it out *before* processing. This code replaces the original `image_dataset_from_directory` with a more flexible `tf.data` pipeline.\n\n---\n\n#### Step 1: Filter Out Corrupted Files\n\nFirst, we define a \"blocklist\" (`corrupted_files_list`) containing the paths of all known bad files.\n\n* We convert this list to a `set` for high-performance lookups (checking if an item is in a set is much faster than checking a list).\n* We get a list of *all* file paths in the directory.\n* We create our final `good_file_paths` list by iterating through all files and keeping only the ones *not* present in the `corrupted_files_set`.\n* Finally, we **sort** this clean list to ensure the image order remains consistent for later steps (like merging with labels).\n\n---\n\n#### Step 2: Build the Custom `tf.data` Pipeline\n\nSince we can no longer use the simple `image_dataset_from_directory` (as it doesn't support an \"exclude\" feature), we build our own pipeline from scratch.\n\n* `tf.data.Dataset.from_tensor_slices`: This is the key. We create a new dataset directly from our clean list of `good_file_paths`. The dataset now only contains paths to valid images.\n* `load_and_preprocess_image`: We define a helper function that does the work `image_dataset_from_directory` used to do for us:\n    1.  `tf.io.read_file`: Reads the image file from its path.\n    2.  `tf.io.decode_jpeg`: Decodes the raw bytes into an image tensor.\n    3.  `tf.image.resize`: Resizes the image to `(224, 224)` using the `lanczos5` method to match our original setup.\n* We then apply this function using `.map()` and add the standard performance optimizations: `.batch()` and `.prefetch()`.\n\n---\n\n#### Step 3: Define Model and Extract Features\n\nThis part is now straightforward.\n\n* We define the exact same `MobileNetV2` `base_model` as before.\n* We call `base_model.predict()`, but this time we pass it our new, custom, and **guaranteed-clean** `image_dataset`.\n* The feature extraction now runs to completion without errors, producing the final `extracted_features` array.","metadata":{}},{"cell_type":"code","source":"# The root directory containing all your images\nroot_dir = \"/kaggle/input/amazon-ml-image-data/images/\"\n\n# Add all corrupted file paths you identify to this list.\ncorrupted_files_list = [\n    '/kaggle/input/amazon-ml-image-data/images/81stS23MRfL.jpg',\n]\n\n# Model and pipeline parameters\nBATCH_SIZE = 64\nIMG_SIZE = (224, 224)\nprint(\"--- Step 1: Filtering out corrupted files ---\")\n\n# Convert the list to a set for very fast lookups\ncorrupted_files_set = set(corrupted_files_list)\n\n# Get the full paths of all files in the directory\nall_file_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir)]\n\n# Create the final, clean list of file paths by excluding the corrupted ones\ngood_file_paths = [path for path in all_file_paths if path not in corrupted_files_set]\n\ngood_file_paths.sort()\n\nprint(f\"Total files found in directory: {len(all_file_paths)}\")\nprint(f\"Number of corrupted files to skip: {len(corrupted_files_set)}\")\nprint(f\"Final number of clean files for processing: {len(good_file_paths)}\")\n\nprint(\"\\n--- Step 2: Building the custom tf.data pipeline ---\")\n\n# Define a function to read, decode, and resize an image from its path\ndef load_and_preprocess_image(path):\n    \n    image = tf.io.read_file(path) # Read the raw file data\n\n    image = tf.io.decode_jpeg(image, channels=3)     # Decode it as a JPEG image with 3 color channels (RGB)\n    \n    image = tf.image.resize(image, IMG_SIZE, method='lanczos5') # Resize the image to the required input size of the model  The interpolation method matches your original code.\n    return image\n\n# Create a TensorFlow Dataset from the clean list of file paths\nimage_dataset = tf.data.Dataset.from_tensor_slices(good_file_paths)\n\n# Apply the preprocessing function to each file path in parallel for efficiency\nimage_dataset = image_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n\n# Batch the images into groups of 64\nimage_dataset = image_dataset.batch(BATCH_SIZE)\n\n# Prefetch the next batch in the background for improved performance\nimage_dataset = image_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n\nprint(\"Custom data pipeline created successfully!\")\nprint(image_dataset)\n\n\nprint(\"\\n--- Step 3: Defining the model and extracting features ---\")\n\n# This is your exact model definition\nbase_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'  # Applies Global Average Pooling to the output\n)\n\n# Run the prediction on the new, clean, and filtered dataset\n# This will now run to completion without errors.\nextracted_features = base_model.predict(image_dataset)\n\nprint(\"\\n--- Feature Extraction Complete! ---\")\nprint(f\"Shape of extracted features: {extracted_features.shape}\")\nprint(f\"This shape means ({extracted_features.shape[0]} images, {extracted_features.shape[1]} features per image)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:46:38.668464Z","iopub.execute_input":"2025-10-13T16:46:38.668690Z","iopub.status.idle":"2025-10-13T16:58:54.589772Z","shell.execute_reply.started":"2025-10-13T16:46:38.668672Z","shell.execute_reply":"2025-10-13T16:58:54.588946Z"}},"outputs":[{"name":"stdout","text":"--- Step 1: Filtering out corrupted files ---\nTotal files found in directory: 72284\nNumber of corrupted files to skip: 1\nFinal number of clean files for processing: 72283\n\n--- Step 2: Building the custom tf.data pipeline ---\n✅ Custom data pipeline created successfully!\n<_PrefetchDataset element_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None)>\n\n--- Step 3: Defining the model and extracting features ---\n\u001b[1m1130/1130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 647ms/step\n\n--- 🚀 Feature Extraction Complete! ---\nShape of extracted features: (72283, 1280)\nThis shape means (72283 images, 1280 features per image)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Text Feature Engineering: TF-IDF\n\nTo use the `catalog_content` text data in our model, we converted it into a numerical format. We'll use **TF-IDF** (Term Frequency-Inverse Document Frequency) for this.\n\n* `TfidfVectorizer(max_features=5000)`: We initialize the vectorizer. This object will convert our text into a matrix of TF-IDF scores.\n    * `max_features=5000`: We are limiting the vocabulary to the **top 5,000 most frequent words** across all catalog entries. This helps control the dimensionality of our data, focusing on the most relevant terms and ignoring very rare or very common (and thus less useful) words.\n\n* `X_text_sparse = vectorizer.fit_transform(train.catalog_content)`:\n    * `fit_transform()` is called on the **training data**.\n    * **fit**: The vectorizer \"learns\" the 5,000-word vocabulary from the `train.catalog_content`.\n    * **transform**: It then converts this training text into a **sparse matrix** (`X_text_sparse`), where each row represents a catalog entry and each column represents one of the 5,000 vocabulary words.\n\n* `test_sparse = vectorizer.transform(test.catalog_content)`:\n    * We *only* call `.transform()` on the **test data**.\n    * This is **critical**: It applies the *same vocabulary* learned from the training data to the test data. This ensures that the resulting columns (features) are consistent and aligned between our training and test sets.","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=5000)\nX_text_sparse = vectorizer.fit_transform(train.catalog_content)\ntest_sparse = vectorizer.transform(test.catalog_content)\nX_text_sparse.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:01:49.526213Z","iopub.execute_input":"2025-10-13T17:01:49.526472Z","iopub.status.idle":"2025-10-13T17:02:03.313965Z","shell.execute_reply.started":"2025-10-13T17:01:49.526453Z","shell.execute_reply":"2025-10-13T17:02:03.313097Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Convert Sparse Matrix to Dense Array\n\nThis line converts our `X_text_sparse` matrix (which was efficiently created by TF-IDF) into a standard, **dense** NumPy array.\n\n* `X_text_sparse`: This is a **sparse matrix**. It saves memory by only storing the *locations* of non-zero values (i.e., the words that are actually present in a document).\n* `.toarray()`: This method expands the sparse matrix into a full, dense array. It fills in all the \"un-stored\" values with zeros.\n\nThe resulting `np_arr_x_sparse` will have the shape `(number_of_documents, 5000)`.\n\n---\n**Important Note on Memory:**\nBe very cautious with this operation. Like our sparse matrix is large (e.g., 100,000 documents $\\times$ 5,000 features), the dense array will require a huge amount of RAM.\n\n* **When to use `.toarray()`**: Only when the resulting array will comfortably fit in your memory *and* the model you are using (like a standard `tf.keras.Dense` layer) *requires* a dense input.\n* **When to avoid it**: If you are using models like XGBoost, LightGBM, or `RidgeRegression`, you should **keep the data in its sparse format**. Those models are optimized to work directly with sparse matrices, which is much faster and more memory-efficient.","metadata":{}},{"cell_type":"code","source":"np_arr_x_sparse = X_text_sparse.toarray()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:29:35.045214Z","iopub.execute_input":"2025-10-13T17:29:35.045468Z","iopub.status.idle":"2025-10-13T17:29:38.935597Z","shell.execute_reply.started":"2025-10-13T17:29:35.045451Z","shell.execute_reply":"2025-10-13T17:29:38.934739Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"### Converting Dense Array to Pandas DataFrame","metadata":{}},{"cell_type":"code","source":"pd_arr = pd.DataFrame(np_arr_x_sparse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:31:53.719073Z","iopub.execute_input":"2025-10-13T17:31:53.719547Z","iopub.status.idle":"2025-10-13T17:31:53.722932Z","shell.execute_reply.started":"2025-10-13T17:31:53.719527Z","shell.execute_reply":"2025-10-13T17:31:53.722088Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"## Vectorizing Test Dataset Catalog_Content Column","metadata":{}},{"cell_type":"code","source":"X_test_sparse_test = vectorizer.transform(test.catalog_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:21:49.847302Z","iopub.execute_input":"2025-10-13T18:21:49.848035Z","iopub.status.idle":"2025-10-13T18:21:56.836731Z","shell.execute_reply.started":"2025-10-13T18:21:49.848003Z","shell.execute_reply":"2025-10-13T18:21:56.836071Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"X_test_sparse_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:22:19.189301Z","iopub.execute_input":"2025-10-13T18:22:19.190101Z","iopub.status.idle":"2025-10-13T18:22:19.194814Z","shell.execute_reply.started":"2025-10-13T18:22:19.190076Z","shell.execute_reply":"2025-10-13T18:22:19.194199Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"<Compressed Sparse Row sparse matrix of dtype 'float64'\n\twith 5548071 stored elements and shape (75000, 5000)>"},"metadata":{}}],"execution_count":53},{"cell_type":"markdown","source":"# Saving Sparse Matrix to NPZ file\n\nWe are saving our processed sparse text features (`X_test_sparse_test`) to a file using `scipy.sparse.save_npz`.\n\nThis is a crucial step for efficiency:\n\n1.  **Saves Space**: It preserves the **sparse format** (like `csr_matrix`), which takes up significantly less disk space than saving the full, dense array would.\n2.  **Saves Time**: It allows us to **reload** this processed data instantly in the future using `load_npz()`. This means we can skip the time-consuming `TfidfVectorizer` step in subsequent runs or in a separate notebook (e.g., for inference).\n\nThe file `sparse_matrix.npz` now contains the complete structure of our sparse test features.","metadata":{}},{"cell_type":"code","source":"from scipy.sparse import csr_matrix, save_npz, load_npz\nfile_path = 'sparse_matrix.npz'\nsave_npz(file_path, X_test_sparse_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:23:21.870787Z","iopub.execute_input":"2025-10-13T18:23:21.871327Z","iopub.status.idle":"2025-10-13T18:23:25.487521Z","shell.execute_reply.started":"2025-10-13T18:23:21.871306Z","shell.execute_reply":"2025-10-13T18:23:25.486715Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"# Saving Extracted Features (Checkpoint)\n\nThis is one of the most important steps for an efficient workflow. The previous step, `base_model.predict()`, was the most computationally expensive part of the entire notebook. It used the **T4 x2 GPU** accelerator to process every single image in our dataset, which can take a significant amount of time.\n\nWe **do not** want to re-run this step every time we open the notebook.\n\nBy using `np.save()`, we save the resulting `extracted_features` array directly to disk as a `.npy` file. This is a standard, highly efficient binary format for storing NumPy arrays.\n\n* `np.save(file_path, extracted_features)`: This command takes our large feature array (e.g., shape `(72283, 1280)`) and writes it to the file `mobilenet_features.npy`.\n* `np.load(file_path)`: This command will be used in all future sessions. It reloads the entire array from the file back into a variable almost instantly.\n\nThis creates a crucial **checkpoint**. In our next notebook session, we can comment out the entire model loading and `model.predict()` sections and just run `np.load()`, allowing us to jump straight to training our final model (like XGBoost or LightGBM) in seconds, **saving valuable time and GPU quota**.","metadata":{}},{"cell_type":"code","source":"\"\"\"import numpy as np\n\n# Assume 'extracted_features' is the NumPy array you got from your model\n# For demonstration, let's create a sample array:\n# extracted_features = np.random.rand(72283, 1280) \n\n# 1. Save the array to a .npy file\nfile_path = 'mobilenet_features.npy'\nnp.save(file_path, extracted_features)\n\nprint(f\"Features saved successfully to '{file_path}'\")\n\n# 2. To use it later, load it back into a variable\nloaded_features = np.load(file_path)\n\nprint(f\"\\n Features loaded successfully from '{file_path}'\")\nprint(f\"Shape of loaded features: {loaded_features.shape}\")\n\n# Optional: Verify that the loaded data is identical to the original\nare_equal = np.array_equal(extracted_features, loaded_features)\nprint(f\"Verification: Loaded data is the same as the original: {are_equal}\")\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:00:57.938578Z","iopub.execute_input":"2025-10-13T17:00:57.938838Z","iopub.status.idle":"2025-10-13T17:00:58.395424Z","shell.execute_reply.started":"2025-10-13T17:00:57.938819Z","shell.execute_reply":"2025-10-13T17:00:58.394746Z"}},"outputs":[{"name":"stdout","text":"✅ Features saved successfully to 'mobilenet_features.npy'\n\n✅ Features loaded successfully from 'mobilenet_features.npy'\nShape of loaded features: (72283, 1280)\nVerification: Loaded data is the same as the original: True\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Re-associating Image Features with Training Data\n\nWe had a \"one-to-many\" problem to solve:\n1.  **Original Data (`train`):** Has many rows (e.g., 75,000+) where multiple rows (products) can share the *same* `image_link`.\n2.  **Extracted Features (`extracted_features`):** This is a NumPy array where we *efficiently* calculated features for each *unique* image (e.g., 72,283 feature vectors).\n\nWe needed to map the correct feature vector back to *every* row in the original `train` DataFrame.\n\n---\n\n#### Step 1: Create a Feature \"Lookup\" DataFrame\n\nWe first created a new, small DataFrame (`features_df`) that acts as a simple key-value \"lookup table.\"\n\n* `'image_link': sorted_unique_links`: We use our clean, sorted list of unique image links. This list **must** be in the exact same order as the feature vectors in `extracted_features`.\n* `'mobilenet_features': list(extracted_features)`: We add our NumPy array of features.\n\nThis `features_df` now has one row for each unique image, linking its `image_link` (the key) to its `mobilenet_features` (the value).\n\n---\n\n#### Step 2: Merge Back to the Original DataFrame\n\nThis is the final, crucial step. We used `pd.merge()` to combine our original `train` DataFrame with our new `features_df` lookup table.\n\n* `pd.merge(train, features_df, ...)`: We are merging `train` (the \"left\" DataFrame) with `features_df` (the \"right\" DataFrame).\n* `on='image_link'`: Specifies that the `image_link` column is the key to join on.\n* `how='left'`: This is the most important part. It instructs pandas to:\n    1.  Keep **all** rows from the \"left\" DataFrame (our original `train` data).\n    2.  For each row in `train`, it looks up its `image_link` in `features_df`.\n    3.  It then \"attaches\" (merges) the corresponding `mobilenet_features` from `features_df` to that row.\n\nThe result (`final_df`) is our complete, enriched dataset. It has the original 75,000+ rows, but now includes a new `mobilenet_features` column containing the correct 1280-dimension vector for each product's image.","metadata":{}},{"cell_type":"code","source":"# our original DataFrame with 75,000 rows (we'll use 6 for the example)\n\n# we already did this part: get unique links and sort them\nsorted_unique_links = sorted(train['image_link'].unique())\n\n# -> ['link_A.jpg', 'link_B.jpg', 'link_C.jpg', 'link_D.jpg']\nsorted_unique_links = sorted_unique_links[:-5]\n\n# our NumPy array of features (simulated) , Shape will be (number_of_unique_links, feature_size)\n\n# Creating the features DataFrame\nfeatures_df = pd.DataFrame({\n    'image_link': sorted_unique_links,\n    'mobilenet_features': list(extracted_features)\n})\n\nprint(\"--- Features DataFrame ---\")\nprint(features_df)\nprint(\"\\n\")\n\n\n# Merging it back with the original DataFrame\nfinal_df = pd.merge(train, features_df, on='image_link', how='left')\n\nprint(\"--- Final Merged DataFrame ---\")\nprint(final_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:17:10.722379Z","iopub.execute_input":"2025-10-13T17:17:10.722640Z","iopub.status.idle":"2025-10-13T17:17:10.857500Z","shell.execute_reply.started":"2025-10-13T17:17:10.722621Z","shell.execute_reply":"2025-10-13T17:17:10.856853Z"}},"outputs":[{"name":"stdout","text":"--- Features DataFrame ---\n                                              image_link  \\\n0      https://m.media-amazon.com/images/I/018kdDYIAY...   \n1      https://m.media-amazon.com/images/I/01O1AwI4pJ...   \n2      https://m.media-amazon.com/images/I/01SCsYMIKj...   \n3      https://m.media-amazon.com/images/I/11+1w3qzdn...   \n4      https://m.media-amazon.com/images/I/11+C-FVBGY...   \n...                                                  ...   \n72278  https://m.media-amazon.com/images/I/A1zEWsomY4...   \n72279  https://m.media-amazon.com/images/I/A1zJmUvrGo...   \n72280  https://m.media-amazon.com/images/I/A1zPpWyb2t...   \n72281  https://m.media-amazon.com/images/I/A1zcbLqB0t...   \n72282  https://m.media-amazon.com/images/I/A1zdqaF5Vo...   \n\n                                      mobilenet_features  \n0      [0.3847377, 0.12714423, 0.002564683, 0.0, 0.0,...  \n1      [0.0, 0.056353927, 0.0, 0.0, 0.0, 0.12231338, ...  \n2      [0.0, 0.03530587, 0.0690719, 0.07184036, 0.051...  \n3      [1.1333392, 0.7011427, 0.051063694, 0.0, 0.0, ...  \n4      [2.2402673, 0.19540116, 0.0, 0.0, 0.85863215, ...  \n...                                                  ...  \n72278  [0.15729806, 0.2606015, 0.0, 0.0, 0.0, 0.0, 1....  \n72279  [0.0058710915, 0.7868472, 0.0072943335, 0.0, 0...  \n72280  [0.05075887, 0.08527423, 0.0, 0.49384627, 0.0,...  \n72281  [0.036130194, 0.6503978, 0.0, 0.0, 0.0, 0.0, 0...  \n72282  [0.06314432, 0.047256876, 0.0, 0.0, 0.0867282,...  \n\n[72283 rows x 2 columns]\n\n\n--- Final Merged DataFrame ---\n       sample_id                                    catalog_content  \\\n0          33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n1         198967  Item Name: Salerno Cookies, The Original Butte...   \n2         261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n3          55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n4         292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n...          ...                                                ...   \n74995      41424  Item Name: ICE BREAKERS Spearmint Sugar Free M...   \n74996      35537  Item Name: Davidson's Organics, Vanilla Essenc...   \n74997     249971  Item Name: Jolly Rancher Hard Candy - Blue Ras...   \n74998     188322  Item Name: Nescafe Dolce Gusto Capsules - CARA...   \n74999     298504  Item Name: Pimenton de la Vera - Picante (2.47...   \n\n                                              image_link   price  \\\n0      https://m.media-amazon.com/images/I/51mo8htwTH...   4.890   \n1      https://m.media-amazon.com/images/I/71YtriIHAA...  13.120   \n2      https://m.media-amazon.com/images/I/51+PFEe-w-...   1.970   \n3      https://m.media-amazon.com/images/I/41mu0HAToD...  30.340   \n4      https://m.media-amazon.com/images/I/41sA037+Qv...  66.490   \n...                                                  ...     ...   \n74995  https://m.media-amazon.com/images/I/81p9PcPsff...  10.395   \n74996  https://m.media-amazon.com/images/I/51DDKoa+mb...  35.920   \n74997  https://m.media-amazon.com/images/I/91R2XCcpUf...  50.330   \n74998  https://m.media-amazon.com/images/I/51W40YU98+...  15.275   \n74999  https://m.media-amazon.com/images/I/81dFnrP6C4...  28.240   \n\n                                      mobilenet_features  \n0      [0.0, 0.3311414, 0.0, 0.0, 0.004351908, 0.0832...  \n1      [0.29220903, 0.7780709, 0.0, 0.0, 0.0, 0.0, 1....  \n2      [0.016326116, 1.9629351, 0.0, 0.0, 0.0, 0.0855...  \n3      [0.0, 0.0, 0.008575429, 0.0, 0.0, 0.0, 1.56927...  \n4      [0.8362831, 0.49924576, 0.00024921066, 0.0, 0....  \n...                                                  ...  \n74995  [0.24348412, 2.5177064, 0.11823583, 0.0, 0.0, ...  \n74996  [0.027533248, 1.2338995, 0.0, 0.0, 0.0, 0.0627...  \n74997  [0.0, 0.8403328, 0.0, 0.0, 0.0, 0.0, 1.9255025...  \n74998  [0.12463549, 2.4297733, 0.0, 0.026566584, 0.0,...  \n74999  [0.12885813, 0.40890658, 0.0, 0.0, 0.0, 0.0, 1...  \n\n[75000 rows x 5 columns]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"### Final Merged Training Dataset","metadata":{}},{"cell_type":"code","source":"final_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:17:31.561593Z","iopub.execute_input":"2025-10-13T17:17:31.561849Z","iopub.status.idle":"2025-10-13T17:17:31.584625Z","shell.execute_reply.started":"2025-10-13T17:17:31.561834Z","shell.execute_reply":"2025-10-13T17:17:31.584012Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"       sample_id                                    catalog_content  \\\n0          33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n1         198967  Item Name: Salerno Cookies, The Original Butte...   \n2         261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n3          55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n4         292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n...          ...                                                ...   \n74995      41424  Item Name: ICE BREAKERS Spearmint Sugar Free M...   \n74996      35537  Item Name: Davidson's Organics, Vanilla Essenc...   \n74997     249971  Item Name: Jolly Rancher Hard Candy - Blue Ras...   \n74998     188322  Item Name: Nescafe Dolce Gusto Capsules - CARA...   \n74999     298504  Item Name: Pimenton de la Vera - Picante (2.47...   \n\n                                              image_link   price  \\\n0      https://m.media-amazon.com/images/I/51mo8htwTH...   4.890   \n1      https://m.media-amazon.com/images/I/71YtriIHAA...  13.120   \n2      https://m.media-amazon.com/images/I/51+PFEe-w-...   1.970   \n3      https://m.media-amazon.com/images/I/41mu0HAToD...  30.340   \n4      https://m.media-amazon.com/images/I/41sA037+Qv...  66.490   \n...                                                  ...     ...   \n74995  https://m.media-amazon.com/images/I/81p9PcPsff...  10.395   \n74996  https://m.media-amazon.com/images/I/51DDKoa+mb...  35.920   \n74997  https://m.media-amazon.com/images/I/91R2XCcpUf...  50.330   \n74998  https://m.media-amazon.com/images/I/51W40YU98+...  15.275   \n74999  https://m.media-amazon.com/images/I/81dFnrP6C4...  28.240   \n\n                                      mobilenet_features  \n0      [0.0, 0.3311414, 0.0, 0.0, 0.004351908, 0.0832...  \n1      [0.29220903, 0.7780709, 0.0, 0.0, 0.0, 0.0, 1....  \n2      [0.016326116, 1.9629351, 0.0, 0.0, 0.0, 0.0855...  \n3      [0.0, 0.0, 0.008575429, 0.0, 0.0, 0.0, 1.56927...  \n4      [0.8362831, 0.49924576, 0.00024921066, 0.0, 0....  \n...                                                  ...  \n74995  [0.24348412, 2.5177064, 0.11823583, 0.0, 0.0, ...  \n74996  [0.027533248, 1.2338995, 0.0, 0.0, 0.0, 0.0627...  \n74997  [0.0, 0.8403328, 0.0, 0.0, 0.0, 0.0, 1.9255025...  \n74998  [0.12463549, 2.4297733, 0.0, 0.026566584, 0.0,...  \n74999  [0.12885813, 0.40890658, 0.0, 0.0, 0.0, 0.0, 1...  \n\n[75000 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>catalog_content</th>\n      <th>image_link</th>\n      <th>price</th>\n      <th>mobilenet_features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33127</td>\n      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n      <td>4.890</td>\n      <td>[0.0, 0.3311414, 0.0, 0.0, 0.004351908, 0.0832...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>198967</td>\n      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n      <td>13.120</td>\n      <td>[0.29220903, 0.7780709, 0.0, 0.0, 0.0, 0.0, 1....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>261251</td>\n      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n      <td>1.970</td>\n      <td>[0.016326116, 1.9629351, 0.0, 0.0, 0.0, 0.0855...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>55858</td>\n      <td>Item Name: Judee’s Blue Cheese Powder 11.25 oz...</td>\n      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n      <td>30.340</td>\n      <td>[0.0, 0.0, 0.008575429, 0.0, 0.0, 0.0, 1.56927...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>292686</td>\n      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n      <td>66.490</td>\n      <td>[0.8362831, 0.49924576, 0.00024921066, 0.0, 0....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>74995</th>\n      <td>41424</td>\n      <td>Item Name: ICE BREAKERS Spearmint Sugar Free M...</td>\n      <td>https://m.media-amazon.com/images/I/81p9PcPsff...</td>\n      <td>10.395</td>\n      <td>[0.24348412, 2.5177064, 0.11823583, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>74996</th>\n      <td>35537</td>\n      <td>Item Name: Davidson's Organics, Vanilla Essenc...</td>\n      <td>https://m.media-amazon.com/images/I/51DDKoa+mb...</td>\n      <td>35.920</td>\n      <td>[0.027533248, 1.2338995, 0.0, 0.0, 0.0, 0.0627...</td>\n    </tr>\n    <tr>\n      <th>74997</th>\n      <td>249971</td>\n      <td>Item Name: Jolly Rancher Hard Candy - Blue Ras...</td>\n      <td>https://m.media-amazon.com/images/I/91R2XCcpUf...</td>\n      <td>50.330</td>\n      <td>[0.0, 0.8403328, 0.0, 0.0, 0.0, 0.0, 1.9255025...</td>\n    </tr>\n    <tr>\n      <th>74998</th>\n      <td>188322</td>\n      <td>Item Name: Nescafe Dolce Gusto Capsules - CARA...</td>\n      <td>https://m.media-amazon.com/images/I/51W40YU98+...</td>\n      <td>15.275</td>\n      <td>[0.12463549, 2.4297733, 0.0, 0.026566584, 0.0,...</td>\n    </tr>\n    <tr>\n      <th>74999</th>\n      <td>298504</td>\n      <td>Item Name: Pimenton de la Vera - Picante (2.47...</td>\n      <td>https://m.media-amazon.com/images/I/81dFnrP6C4...</td>\n      <td>28.240</td>\n      <td>[0.12885813, 0.40890658, 0.0, 0.0, 0.0, 0.0, 1...</td>\n    </tr>\n  </tbody>\n</table>\n<p>75000 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"final_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:19:01.237849Z","iopub.execute_input":"2025-10-13T17:19:01.238144Z","iopub.status.idle":"2025-10-13T17:19:01.292091Z","shell.execute_reply.started":"2025-10-13T17:19:01.238125Z","shell.execute_reply":"2025-10-13T17:19:01.291279Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 75000 entries, 0 to 74999\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   sample_id           75000 non-null  int64  \n 1   catalog_content     75000 non-null  object \n 2   image_link          75000 non-null  object \n 3   price               75000 non-null  float64\n 4   mobilenet_features  74995 non-null  object \ndtypes: float64(1), int64(1), object(3)\nmemory usage: 2.9+ MB\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Saving Merged Training Dataset to CSV","metadata":{}},{"cell_type":"code","source":"final_df.to_csv(\"final_df.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:30:40.961705Z","iopub.execute_input":"2025-10-13T17:30:40.962436Z","iopub.status.idle":"2025-10-13T17:30:49.519403Z","shell.execute_reply.started":"2025-10-13T17:30:40.962411Z","shell.execute_reply":"2025-10-13T17:30:49.518813Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"### Concatanated DataFrame","metadata":{}},{"cell_type":"code","source":"concat_df = pd.concat([final_df.drop([\"catalog_content\",'image_link'],axis=1), pd_arr],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:35:06.643696Z","iopub.execute_input":"2025-10-13T17:35:06.643951Z","iopub.status.idle":"2025-10-13T17:35:17.290525Z","shell.execute_reply.started":"2025-10-13T17:35:06.643933Z","shell.execute_reply":"2025-10-13T17:35:17.289717Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"### Saving Concatanated DataFrame to CSV","metadata":{}},{"cell_type":"code","source":"concat_df.to_csv(\"concat_df.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:35:22.299490Z","iopub.execute_input":"2025-10-13T17:35:22.300184Z","iopub.status.idle":"2025-10-13T17:38:57.866202Z","shell.execute_reply.started":"2025-10-13T17:35:22.300160Z","shell.execute_reply":"2025-10-13T17:38:57.865601Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"### Features Shape Extracted from a single image using MobileNet","metadata":{}},{"cell_type":"code","source":"final_df.mobilenet_features[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:38:57.867510Z","iopub.execute_input":"2025-10-13T17:38:57.867748Z","iopub.status.idle":"2025-10-13T17:38:57.872470Z","shell.execute_reply.started":"2025-10-13T17:38:57.867731Z","shell.execute_reply":"2025-10-13T17:38:57.871934Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"(1280,)"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"# Final Feature Engineering: Expanding Image Features\n\nOur `mobilenet_features` column is currently \"packed\" – each cell contains a list or vector of 1280 numbers. To make these features usable for models like XGBoost or LightGBM, we must \"unpack\" them, so that each of the 1280 features gets its **own column**.\n\n---\n\n#### Step 1: Expand the Feature Column\n\n`expanded_features = concat_df['mobilenet_features'].apply(pd.Series)`\n\nWe used the `.apply(pd.Series)` method on the `mobilenet_features` column. This powerful pandas function takes each list-like item (of 1280 features) and explodes it horizontally into a new DataFrame.\n\n* **Before**: One column named `mobilenet_features`.\n* **After**: A new DataFrame (`expanded_features`) with **1280 columns** (named `0`, `1`, `2`, ..., `1279`) and the same number of rows.\n\n---\n\n#### Step 2: Rename New Columns\n\n`expanded_features.columns = [f'f_{i}' for i in expanded_features.columns]`\n\nThe new columns are named with plain integers (0, 1, 2...), which can be problematic for some models (like LightGBM). We rename them to be descriptive and unique, such as `f_0`, `f_1`, `f_2`, and so on.\n\n---\n\n#### Step 3: Concatenate Back to the Main DataFrame\n\n`full_final = pd.concat([..., expanded_features], axis=1)`\n\nFinally, we combined our original data with our new expanded features.\n\n1.  `concat_df.drop('mobilenet_features', axis=1)`: We take our DataFrame but **drop** the original \"packed\" `mobilenet_features` column, as it's now redundant.\n2.  `expanded_features`: This is our new DataFrame containing the 1280 individual feature columns.\n3.  `axis=1`: This tells `pd.concat` to join the two DataFrames **side-by-side** (horizontally).\n\nThe `full_final` DataFrame is now our complete, \"flat\" dataset, ready for modeling. It contains all text features and all 1280 image features as separate columns.","metadata":{}},{"cell_type":"code","source":"expanded_features = concat_df['mobilenet_features'].apply(pd.Series)\n\nexpanded_features.columns = [f'f_{i}' for i in expanded_features.columns]\n\n\nfull_final = pd.concat([concat_df.drop('mobilenet_features', axis=1), expanded_features], axis=1)\n\n\nprint(\"Final DataFrame with Expanded Features:\")\nprint(full_final.head())\nprint(f\"\\nFinal Shape: {full_final.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:44:03.551559Z","iopub.execute_input":"2025-10-13T17:44:03.551823Z","iopub.status.idle":"2025-10-13T17:44:30.676240Z","shell.execute_reply.started":"2025-10-13T17:44:03.551804Z","shell.execute_reply":"2025-10-13T17:44:30.675576Z"}},"outputs":[{"name":"stdout","text":"Final DataFrame with Expanded Features:\n   sample_id  price    0    1    2    3    4    5    6    7  ...    f_1270  \\\n0      33127   4.89  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.461782   \n1     198967  13.12  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  2.485705   \n2     261251   1.97  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.103680   \n3      55858  30.34  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.448067   \n4     292686  66.49  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  2.490304   \n\n     f_1271    f_1272    f_1273    f_1274    f_1275    f_1276  f_1277  \\\n0  0.185137  0.047164  0.000000  3.179038  0.000000  0.422553     0.0   \n1  0.027164  2.033786  0.126224  3.105233  0.006095  0.367020     0.0   \n2  0.003377  0.000000  1.435634  1.427566  0.004048  0.404094     0.0   \n3  0.000000  0.733142  0.062608  1.937328  0.000000  0.349168     0.0   \n4  0.000000  1.652621  0.000000  0.900392  0.000000  0.211833     0.0   \n\n     f_1278    f_1279  \n0  0.220657  0.000000  \n1  1.337896  0.191102  \n2  0.140338  0.269307  \n3  0.110786  0.412674  \n4  1.239623  0.049370  \n\n[5 rows x 6282 columns]\n\nFinal Shape: (75000, 6282)\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"### Saving Finally created dataframe to CSV ( Checkpoint )","metadata":{}},{"cell_type":"code","source":"full_final.to_csv(\"Full_final.csv\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:45:59.603525Z","iopub.execute_input":"2025-10-13T17:45:59.604106Z","iopub.status.idle":"2025-10-13T17:51:51.415287Z","shell.execute_reply.started":"2025-10-13T17:45:59.604079Z","shell.execute_reply":"2025-10-13T17:51:51.414485Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"### Creating CSR Matrix of the final dataframe","metadata":{}},{"cell_type":"code","source":"full_final_csr = csr_matrix(full_final)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:55:42.556493Z","iopub.execute_input":"2025-10-13T17:55:42.556806Z","iopub.status.idle":"2025-10-13T17:56:09.490920Z","shell.execute_reply.started":"2025-10-13T17:55:42.556783Z","shell.execute_reply":"2025-10-13T17:56:09.490316Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"### Splitting the Dataset on the Train and Evaluation Sets","metadata":{}},{"cell_type":"code","source":"X_train, X_eval, y_train, y_eval = train_test_split(full_final_csr, train.price)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:57:27.692663Z","iopub.execute_input":"2025-10-13T17:57:27.692934Z","iopub.status.idle":"2025-10-13T17:57:27.986259Z","shell.execute_reply.started":"2025-10-13T17:57:27.692914Z","shell.execute_reply":"2025-10-13T17:57:27.985598Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"### Training XGB_Regressor model on","metadata":{}},{"cell_type":"code","source":"xgb_regressor = XGBRegressor()\n\n# Train the model on your entire training data\nxgb_regressor.fit(X_train, y_train,verbose=True) # Set to True to see training progress","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:59:58.426342Z","iopub.execute_input":"2025-10-13T17:59:58.426874Z","iopub.status.idle":"2025-10-13T18:01:11.571959Z","shell.execute_reply.started":"2025-10-13T17:59:58.426855Z","shell.execute_reply":"2025-10-13T18:01:11.571345Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":49},{"cell_type":"markdown","source":"# Evaluate Model Performance (SMAPE)\n\nNow we evaluate our trained `xgb_regressor` on the hold-out validation set (`X_eval`).\n\n#### 1. Generate Predictions\n`y_pred = xgb_regressor.predict(X_eval)`\n\n#### 2. Define the Evaluation Metric: SMAPE\nWe must define the **SMAPE** (Symmetric Mean Absolute Percentage Error) function, as it is not built-in.\n\nThe formula is: $SMAPE = \\frac{100}{n} \\sum_{i=1}^{n} \\frac{2 \\cdot |F_i - A_i|}{|A_i| + |F_i|}$\n\n* `epsilon = np.finfo(float).eps`: This is a critical addition. `epsilon` is the smallest possible positive number. We add it to the denominator to prevent a **divide-by-zero error** if both the actual and predicted value are 0.\n\n#### 3. Calculate and Print the Final Score\nFinally, we call our custom `smape` function to compare the true validation labels (`y_eval`) against our model's predictions (`y_pred`).\n\n`print(\"SMAPE:\", smape(y_eval, y_pred))`","metadata":{}},{"cell_type":"code","source":"y_pred = xgb_regressor.predict(X_eval)\ndef smape(actual, forecast):\n    epsilon = np.finfo(float).eps\n    return 100 / len(actual) * np.sum(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast) + epsilon))\n\nprint(\"SMAPE:\", smape(y_eval, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:02:27.389945Z","iopub.execute_input":"2025-10-13T18:02:27.390644Z","iopub.status.idle":"2025-10-13T18:02:27.539042Z","shell.execute_reply.started":"2025-10-13T18:02:27.390623Z","shell.execute_reply":"2025-10-13T18:02:27.538309Z"}},"outputs":[{"name":"stdout","text":"SMAPE: 1.4315532046788857\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"# Save the Trained Model\n\nThis is the final and one of the most important steps of our training pipeline. We've spent a lot of time and computational resources training our `xgb_regressor` model. We must now **save (or \"serialize\") this trained object** to disk so we can use it later without retraining.\n\n* **Why `joblib`?**: We use `joblib.dump()` because it is highly efficient for saving Python objects that contain large NumPy arrays, which is exactly what our trained XGBoost model is. It's generally preferred over other methods like `pickle` for scikit-learn-compatible models.\n\n* **What it does**: The command saves the entire model—including all its learned internal parameters, feature importances, and configuration—into a single file named `xgbregressor_model.joblib`.\n\n* **Next Steps**: This file is our final \"artifact.\" We can now download it, or (more commonly) create a new, separate \"Inference Notebook.\" In that notebook, we will simply load this file using `joblib.load()` and use it to generate predictions on the *actual competition test set*.","metadata":{}},{"cell_type":"code","source":"joblib_filename = 'xgbregressor_model.joblib'\njoblib.dump(xgb_regressor, joblib_filename)\nprint(f\"Model saved successfully to '{joblib_filename}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:08:35.882182Z","iopub.execute_input":"2025-10-13T18:08:35.882489Z","iopub.status.idle":"2025-10-13T18:08:35.891710Z","shell.execute_reply.started":"2025-10-13T18:08:35.882471Z","shell.execute_reply":"2025-10-13T18:08:35.890619Z"}},"outputs":[{"name":"stdout","text":"✅ Model saved successfully to 'xgbregressor_model.joblib'\n","output_type":"stream"}],"execution_count":51}]}